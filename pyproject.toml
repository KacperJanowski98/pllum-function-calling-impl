[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "pllum-function-calling"
version = "0.1.0"
description = "Tools for analyzing the Salesforce/xlam-function-calling-60k dataset and fine-tuning PLLuM for function calling"
requires-python = ">=3.10"
dependencies = [
    "datasets>=2.17.0",
    "huggingface-hub>=0.21.0",
    "pandas>=2.1.0",
    "matplotlib>=3.8.0",
    "seaborn>=0.13.0",
    "ipykernel>=6.0.0",
    "notebook>=7.0.0",
    "python-dotenv>=1.0.0",
    "tqdm>=4.66.0",
    "plotly>=5.18.0",
    "googletrans==4.0.0-rc1",  # For translating English to Polish
    "langdetect>=1.0.9",
    # Fine-tuning dependencies
    "torch>=2.0.0",           # Make sure to install with CUDA support (pip install torch --index-url https://download.pytorch.org/whl/cu118)
    "transformers>=4.34.0",
    "peft>=0.5.0",            # For QLoRA fine-tuning
    "accelerate>=0.23.0",     # Required for efficient training
    "bitsandbytes>=0.40.0",   # For 4-bit quantization
    "einops>=0.6.1",          # Required by some transformer models
    "unsloth>=2025.1.1",         # Unsloth framework for optimized training
    "sentencepiece>=0.1.99",  # For tokenization
    "optimum>=1.12.0",        # HuggingFace optimization
    "tensorboard>=2.14.0",    # For tracking training metrics
    "evaluate>=0.4.0",        # For model evaluation
    "rouge-score>=0.1.2",     # For evaluating text generation
    "safetensors>=0.3.3",     # For model safetensors
    "nvidia-ml-py>=12.0.0",   # For GPU monitoring
]

[project.optional-dependencies]
dev = [
    "black>=23.0.0",
    "isort>=5.12.0",
    "pytest>=7.4.0",
]

# Add a CUDA-specific dependency group
cuda = [
    "triton>=2.0.0",          # For optimized CUDA operations
    "flash-attn>=2.3.0",      # Flash Attention for faster training
]

[tool.setuptools]
packages = ["src"]

[tool.black]
line-length = 88
target-version = ["py310"]

[tool.isort]
profile = "black"
line_length = 88
