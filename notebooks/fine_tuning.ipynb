{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning PLLuM 8B for Function Calling\n",
    "\n",
    "This notebook implements the fine-tuning of the Polish language model [CYFRAGOVPL/Llama-PLLuM-8B-instruct](https://huggingface.co/CYFRAGOVPL/Llama-PLLuM-8B-instruct) for function calling tasks using a dataset of examples in both Polish and English.\n",
    "\n",
    "We use the following techniques:\n",
    "- **QLoRA** (Quantized Low-Rank Adaptation) with 4-bit quantization for memory efficiency\n",
    "- **Unsloth** framework for optimized training speed\n",
    "- Mixed dataset with both Polish and English examples\n",
    "\n",
    "The fine-tuning adapts the model to understand the specific format of function calling requests and to generate proper JSON responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install/update dependencies if needed\n",
    "# Note: You should install PyTorch with CUDA support first using:\n",
    "# pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q -U unsloth bitsandbytes sentencepiece nvidia-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# Import our fine-tuning utilities\n",
    "from src.fine_tuning import (\n",
    "    PLLuMFineTuningConfig,\n",
    "    setup_model_and_tokenizer,\n",
    "    prepare_dataset,\n",
    "    train_model,\n",
    "    format_function_calling_prompt,\n",
    "    generate_function_call,\n",
    "    check_cuda_compatibility,\n",
    ")\n",
    "from src.auth import login  # For Hugging Face authentication\n",
    "from src.dataset import parse_json_entry\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU and CUDA Compatibility\n",
    "\n",
    "Before proceeding with fine-tuning, let's verify that we have a compatible GPU with CUDA properly configured. This notebook is designed for use with an NVIDIA RTX 4060 or similar GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run CUDA compatibility check\n",
    "cuda_info = check_cuda_compatibility()\n",
    "\n",
    "if not cuda_info['cuda_available']:\n",
    "    print(\"WARNING: CUDA is not available! Fine-tuning will be extremely slow on CPU.\")\n",
    "    print(\"Please make sure you have an NVIDIA GPU and have installed PyTorch with CUDA support.\")\n",
    "    print(\"You can install PyTorch with CUDA using: pip install torch --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    # Optionally stop execution\n",
    "    # raise RuntimeError(\"CUDA is required for fine-tuning\")\n",
    "else:\n",
    "    print(f\"\\n✅ CUDA is available with version {cuda_info['cuda_version']}\")\n",
    "    for i, device in enumerate(cuda_info['devices']):\n",
    "        print(f\"\\nGPU {i}: {device['name']}\")\n",
    "        print(f\"  Memory: {device['total_memory_gb']:.2f} GB total\")\n",
    "        \n",
    "        if 'memory_free_gb' in device:\n",
    "            print(f\"  Free memory: {device['memory_free_gb']:.2f} GB\")\n",
    "            print(f\"  Used memory: {device['memory_used_gb']:.2f} GB\")\n",
    "            \n",
    "            # Check if there's enough free memory (at least 6GB recommended for 8B model with QLoRA)\n",
    "            if device['memory_free_gb'] < 6.0:\n",
    "                print(f\"⚠️ Warning: Only {device['memory_free_gb']:.2f} GB free memory detected.\")\n",
    "                print(\"   You may encounter out-of-memory errors during fine-tuning.\")\n",
    "                print(\"   Consider reducing batch size, sequence length, or closing other applications.\")\n",
    "            else:\n",
    "                print(f\"✅ Sufficient free memory detected ({device['memory_free_gb']:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify PyTorch was installed with CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    # Run a simple test to verify CUDA is working\n",
    "    try:\n",
    "        print(\"Running CUDA test...\")\n",
    "        x = torch.rand(10, 10).cuda()\n",
    "        y = torch.rand(10, 10).cuda()\n",
    "        z = x @ y  # Matrix multiplication\n",
    "        print(f\"CUDA test result shape: {z.shape}\")\n",
    "        print(\"✅ CUDA test passed!\")\n",
    "        \n",
    "        # Test triton if installed (optional acceleration library)\n",
    "        try:\n",
    "            import triton\n",
    "            print(\"✅ Triton is installed for additional CUDA optimizations\")\n",
    "        except ImportError:\n",
    "            print(\"ℹ️ Triton is not installed. For additional optimizations, install with: pip install triton\")\n",
    "            \n",
    "        # Test flash-attention if installed\n",
    "        try:\n",
    "            import flash_attn\n",
    "            print(\"✅ Flash Attention is installed for faster training\")\n",
    "        except ImportError:\n",
    "            print(\"ℹ️ Flash Attention is not installed. For faster training, install with: pip install flash-attn\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CUDA test failed: {str(e)}\")\n",
    "        print(\"This may indicate a problem with your CUDA installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine the Dataset\n",
    "\n",
    "We'll load the translated dataset and examine it to understand its structure.\n",
    "\n",
    "**Note:** In this dataset, the `tools` and `answers` fields are stored as JSON strings that need to be parsed with `json.loads()`. This is the expected format according to the dataset documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Path to the translated dataset\n",
    "DATASET_PATH = \"../data/translated_dataset.json\"\n",
    "\n",
    "# Check if the dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}. Please run create_translated_dataset.ipynb first.\")\n",
    "\n",
    "# Load the dataset\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine dataset structure\n",
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "if len(dataset) > 0:\n",
    "    print(f\"First item type: {type(dataset[0])}\")\n",
    "    if isinstance(dataset[0], dict):\n",
    "        print(f\"First item keys: {list(dataset[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine a few examples with understanding that tools and answers are JSON strings\n",
    "def print_example(example, idx=0):\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    \n",
    "    # Handle tools as a JSON string (intended format)\n",
    "    if 'tools' in example:\n",
    "        tools_type = type(example['tools'])\n",
    "        if isinstance(example['tools'], str):\n",
    "            # Parse the JSON string to show tool count\n",
    "            try:\n",
    "                parsed_tools = json.loads(example['tools'])\n",
    "                print(f\"Tools: JSON string containing {len(parsed_tools)} tool(s)\")\n",
    "                # Print first tool if available\n",
    "                if len(parsed_tools) > 0:\n",
    "                    print(f\"First tool: {parsed_tools[0]['name']}\")\n",
    "                    print(f\"Description: {parsed_tools[0]['description'][:50]}...\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Tools: Invalid JSON string\")\n",
    "        else:\n",
    "            print(f\"Tools has unexpected type: {tools_type}\")\n",
    "    \n",
    "    # Handle answers as a JSON string (intended format)\n",
    "    if 'answers' in example:\n",
    "        answers_type = type(example['answers'])\n",
    "        if isinstance(example['answers'], str):\n",
    "            # Parse the JSON string to show answer count\n",
    "            try:\n",
    "                parsed_answers = json.loads(example['answers'])\n",
    "                print(f\"Answers: JSON string containing {len(parsed_answers)} answer(s)\")\n",
    "                # Print first answer if available\n",
    "                if len(parsed_answers) > 0:\n",
    "                    print(f\"First answer uses tool: {parsed_answers[0]['name']}\")\n",
    "                    print(f\"With arguments: {parsed_answers[0]['arguments']}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Answers: Invalid JSON string\")\n",
    "        else:\n",
    "            print(f\"Answers has unexpected type: {answers_type}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print examples\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print_example(dataset[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the format_function_calling_prompt function with our dataset\n",
    "# This will parse the JSON strings automatically\n",
    "if len(dataset) > 0:\n",
    "    example_prompt = format_function_calling_prompt(dataset[0])\n",
    "    print(\"Example formatted prompt:\")\n",
    "    print(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Fine-tuning Parameters\n",
    "\n",
    "Set up the fine-tuning configuration with parameters optimized for an RTX 4060 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a timestamped output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_OUTPUT_DIR = f\"../models/pllum-function-calling-{timestamp}\"\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configure fine-tuning parameters\n",
    "config = PLLuMFineTuningConfig(\n",
    "    model_name_or_path=\"CYFRAGOVPL/Llama-PLLuM-8B-instruct\",\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    \n",
    "    # QLoRA settings\n",
    "    lora_r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha\n",
    "    lora_dropout=0.05,\n",
    "    use_4bit=True,  # Use 4-bit quantization for memory efficiency\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normal Float 4-bit quantization\n",
    "    use_nested_quant=False,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=2,  # Increase effective batch size\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Logging & Saving\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Dataset parameters\n",
    "    max_seq_length=1024,  # Maximum sequence length\n",
    "    dataset_path=DATASET_PATH,\n",
    "    \n",
    "    # CUDA settings\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Adjust batch size if we detect limited GPU memory\n",
    "if torch.cuda.is_available() and 'devices' in cuda_info and len(cuda_info['devices']) > 0:\n",
    "    if 'memory_free_gb' in cuda_info['devices'][0] and cuda_info['devices'][0]['memory_free_gb'] < 6.0:\n",
    "        original_batch_size = config.per_device_train_batch_size\n",
    "        config.per_device_train_batch_size = 2  # Reduce batch size for low memory GPUs\n",
    "        config.gradient_accumulation_steps = 4  # Increase gradient accumulation\n",
    "        print(f\"⚠️ Limited GPU memory detected. Reducing batch size from {original_batch_size} to {config.per_device_train_batch_size}\")\n",
    "        print(f\"   and increasing gradient accumulation steps to {config.gradient_accumulation_steps}\")\n",
    "\n",
    "# Save the configuration to the model directory for future reference\n",
    "config_dict = {k: str(v) if isinstance(v, Path) else v for k, v in vars(config).items()}\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"config.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Setup the model with QLoRA and Unsloth optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = setup_model_and_tokenizer(config)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare the dataset\n",
    "print(\"Preparing dataset...\")\n",
    "train_dataset = prepare_dataset(\n",
    "    dataset_path=config.dataset_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_seq_length\n",
    ")\n",
    "print(f\"Dataset prepared with {len(train_dataset['input_ids'])} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the Model\n",
    "\n",
    "This is the main training process. It will take several hours depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the training\n",
    "print(f\"Starting fine-tuning process. Model will be saved to {config.output_dir}\")\n",
    "print(\"This may take several hours depending on your hardware.\")\n",
    "\n",
    "# Start training\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-tuned Model\n",
    "\n",
    "Let's test our model with a few examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the model with examples from the dataset\n",
    "def test_model_with_example(example_idx=0):\n",
    "    example = dataset[example_idx]\n",
    "    \n",
    "    query = example['query']\n",
    "    \n",
    "    # Parse tools and answers from JSON strings\n",
    "    tools = json.loads(example['tools'])\n",
    "    expected_answers = json.loads(example['answers'])\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\nAvailable tools:\")\n",
    "    for i, tool in enumerate(tools):\n",
    "        print(f\"{i+1}. {tool['name']}: {tool['description']}\")\n",
    "    \n",
    "    print(\"\\nExpected answer:\")\n",
    "    print(json.dumps(expected_answers, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    print(\"\\nGenerating function call...\")\n",
    "    generated = generate_function_call(\n",
    "        model=trained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        query=query,\n",
    "        tools=tools,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenerated answer:\")\n",
    "    print(json.dumps(generated, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test with a few examples\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    generated = test_model_with_example(i)\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor GPU Usage During Inference\n",
    "\n",
    "Let's check the GPU usage during inference to help optimize your settings for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        \n",
    "        # Get memory info before inference\n",
    "        mem_info_before = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"GPU memory before inference: {mem_info_before.used / 1e9:.2f} GB used\")\n",
    "        \n",
    "        # Run a complex inference with sample from dataset\n",
    "        if len(dataset) > 0:\n",
    "            example = dataset[0]\n",
    "            query = example['query']\n",
    "            tools = json.loads(example['tools'])  # Parse JSON string\n",
    "            \n",
    "            generate_function_call(\n",
    "                model=trained_model,\n",
    "                tokenizer=tokenizer,\n",
    "                query=query,\n",
    "                tools=tools,\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=1024  # Longer generation to stress test\n",
    "            )\n",
    "        \n",
    "        # Get memory info after inference\n",
    "        mem_info_after = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"GPU memory after inference: {mem_info_after.used / 1e9:.2f} GB used\")\n",
    "        print(f\"Memory used by inference: {(mem_info_after.used - mem_info_before.used) / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Get GPU utilization\n",
    "        util_rates = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        print(f\"GPU utilization: {util_rates.gpu}%\")\n",
    "        print(f\"Memory utilization: {util_rates.memory}%\")\n",
    "        \n",
    "        pynvml.nvmlShutdown()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not monitor GPU: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Final Model Summary\n",
    "\n",
    "Let's create a summary file with information about the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a summary file\n",
    "summary = {\n",
    "    \"model_name\": config.model_name_or_path,\n",
    "    \"fine_tuned_model_path\": config.output_dir,\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset\": {\n",
    "        \"path\": config.dataset_path,\n",
    "        \"num_examples\": len(train_dataset['input_ids']),\n",
    "    },\n",
    "    \"training_parameters\": {\n",
    "        \"epochs\": config.num_train_epochs,\n",
    "        \"batch_size\": config.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
    "        \"effective_batch_size\": config.per_device_train_batch_size * config.gradient_accumulation_steps,\n",
    "        \"learning_rate\": config.learning_rate,\n",
    "        \"lora_r\": config.lora_r,\n",
    "        \"lora_alpha\": config.lora_alpha,\n",
    "        \"max_seq_length\": config.max_seq_length,\n",
    "        \"use_4bit\": config.use_4bit,\n",
    "    },\n",
    "    \"hardware\": {\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"None\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save the summary\n",
    "with open(os.path.join(config.output_dir, \"training_summary.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Training summary saved to {os.path.join(config.output_dir, 'training_summary.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The PLLuM 8B model has been successfully fine-tuned for function calling using QLoRA techniques and the Unsloth framework for optimization. The model can now be used to parse queries and generate appropriate function calls in both Polish and English languages.\n",
    "\n",
    "To use the fine-tuned model in your applications, check the `test_model.ipynb` notebook for examples of how to load and integrate the model into your pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
