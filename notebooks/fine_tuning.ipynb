{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning PLLuM 8B for Function Calling\n",
    "\n",
    "This notebook implements the fine-tuning of the Polish language model [CYFRAGOVPL/Llama-PLLuM-8B-instruct](https://huggingface.co/CYFRAGOVPL/Llama-PLLuM-8B-instruct) for function calling tasks using a dataset of examples in both Polish and English.\n",
    "\n",
    "We use the following techniques:\n",
    "- **QLoRA** (Quantized Low-Rank Adaptation) with 4-bit quantization for memory efficiency\n",
    "- **Unsloth** framework for optimized training speed\n",
    "- Mixed dataset with both Polish and English examples\n",
    "\n",
    "The fine-tuning adapts the model to understand the specific format of function calling requests and to generate proper JSON responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install/update dependencies if needed\n",
    "!pip install -q -U unsloth bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# Import our fine-tuning utilities\n",
    "from src.fine_tuning import (\n",
    "    PLLuMFineTuningConfig,\n",
    "    setup_model_and_tokenizer,\n",
    "    prepare_dataset,\n",
    "    train_model,\n",
    "    format_function_calling_prompt,\n",
    "    generate_function_call,\n",
    ")\n",
    "from src.auth import login  # For Hugging Face authentication\n",
    "from src.dataset import parse_json_entry\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU availability\n",
    "\n",
    "Make sure we have a GPU available for training. This notebook is designed for use with an NVIDIA RTX 4060."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Print CUDA version\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Fine-tuning will be very slow without a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine the Dataset\n",
    "\n",
    "We'll load the translated dataset and examine it to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Path to the translated dataset\n",
    "DATASET_PATH = \"../data/translated_dataset.json\"\n",
    "\n",
    "# Check if the dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}. Please run create_translated_dataset.ipynb first.\")\n",
    "\n",
    "# Load the dataset\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine a few examples\n",
    "def print_example(example, idx=0):\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(f\"Tools: {len(example['tools'])} available\")\n",
    "    \n",
    "    # Show the first tool\n",
    "    if len(example['tools']) > 0:\n",
    "        print(f\"First tool: {example['tools'][0]['name']}\")\n",
    "        print(f\"Description: {example['tools'][0]['description']}\")\n",
    "        \n",
    "    print(f\"Answers: {len(example['answers'])} answers\")\n",
    "    \n",
    "    # Show the first answer\n",
    "    if len(example['answers']) > 0:\n",
    "        print(f\"First answer uses tool: {example['answers'][0]['name']}\")\n",
    "        print(f\"With arguments: {example['answers'][0]['arguments']}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print a few examples\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print_example(dataset[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Let's check how the formatted prompt will look\n",
    "example_prompt = format_function_calling_prompt(dataset[0])\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Fine-tuning Parameters\n",
    "\n",
    "Set up the fine-tuning configuration with parameters optimized for an RTX 4060 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a timestamped output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_OUTPUT_DIR = f\"../models/pllum-function-calling-{timestamp}\"\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configure fine-tuning parameters\n",
    "config = PLLuMFineTuningConfig(\n",
    "    model_name_or_path=\"CYFRAGOVPL/Llama-PLLuM-8B-instruct\",\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    \n",
    "    # QLoRA settings\n",
    "    lora_r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha\n",
    "    lora_dropout=0.05,\n",
    "    use_4bit=True,  # Use 4-bit quantization for memory efficiency\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normal Float 4-bit quantization\n",
    "    use_nested_quant=False,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=2,  # Increase effective batch size\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Logging & Saving\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Dataset parameters\n",
    "    max_seq_length=1024,  # Maximum sequence length\n",
    "    dataset_path=DATASET_PATH,\n",
    ")\n",
    "\n",
    "# Save the configuration to the model directory for future reference\n",
    "config_dict = {k: str(v) if isinstance(v, Path) else v for k, v in vars(config).items()}\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"config.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Setup the model with QLoRA and Unsloth optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = setup_model_and_tokenizer(config)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare the dataset\n",
    "print(\"Preparing dataset...\")\n",
    "train_dataset = prepare_dataset(\n",
    "    dataset_path=config.dataset_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_seq_length\n",
    ")\n",
    "print(f\"Dataset prepared with {len(train_dataset['input_ids'])} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the Model\n",
    "\n",
    "This is the main training process. It will take several hours depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the training\n",
    "print(f\"Starting fine-tuning process. Model will be saved to {config.output_dir}\")\n",
    "print(\"This may take several hours depending on your hardware.\")\n",
    "\n",
    "# Start training\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-tuned Model\n",
    "\n",
    "Let's test our model with a few examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the model with examples from the dataset\n",
    "def test_model_with_example(example_idx=0):\n",
    "    example = dataset[example_idx]\n",
    "    \n",
    "    query = example['query']\n",
    "    tools = example['tools']\n",
    "    expected_answers = example['answers']\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\nAvailable tools:\")\n",
    "    for i, tool in enumerate(tools):\n",
    "        print(f\"{i+1}. {tool['name']}: {tool['description']}\")\n",
    "    \n",
    "    print(\"\\nExpected answer:\")\n",
    "    print(json.dumps(expected_answers, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    print(\"\\nGenerating function call...\")\n",
    "    generated = generate_function_call(\n",
    "        model=trained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        query=query,\n",
    "        tools=tools,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenerated answer:\")\n",
    "    print(json.dumps(generated, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test with a few examples\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    generated = test_model_with_example(i)\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Final Model Summary\n",
    "\n",
    "Let's create a summary file with information about the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a summary file\n",
    "summary = {\n",
    "    \"model_name\": config.model_name_or_path,\n",
    "    \"fine_tuned_model_path\": config.output_dir,\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset\": {\n",
    "        \"path\": config.dataset_path,\n",
    "        \"num_examples\": len(train_dataset['input_ids']),\n",
    "    },\n",
    "    \"training_parameters\": {\n",
    "        \"epochs\": config.num_train_epochs,\n",
    "        \"batch_size\": config.per_device_train_batch_size,\n",
    "        \"learning_rate\": config.learning_rate,\n",
    "        \"lora_r\": config.lora_r,\n",
    "        \"lora_alpha\": config.lora_alpha,\n",
    "        \"max_seq_length\": config.max_seq_length,\n",
    "    },\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"None\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save the summary\n",
    "with open(os.path.join(config.output_dir, \"training_summary.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Training summary saved to {os.path.join(config.output_dir, 'training_summary.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The PLLuM 8B model has been successfully fine-tuned for function calling using QLoRA techniques and the Unsloth framework for optimization. The model can now be used to parse queries and generate appropriate function calls in both Polish and English languages.\n",
    "\n",
    "To use the fine-tuned model in your applications, check the `test_model.ipynb` notebook for examples of how to load and integrate the model into your pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
